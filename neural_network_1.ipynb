{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Deep Learning? Briefly describe how it evolved and how it differs from traditional machine learning.\n",
        "\n",
        "\n",
        "Answer:\n",
        "Deep Learning is a subset of Machine Learning that uses artificial neural networks with many layers to automatically learn complex patterns from large data.\n",
        "It evolved from simple perceptrons (1950s) → neural networks (1980s) → deep neural networks (2000s) due to more data, GPUs, and better algorithms.\n",
        "Unlike traditional ML, which needs manual feature extraction, Deep Learning automatically learns features from raw data.\n",
        "\n",
        "Question 2: Explain the basic architecture and functioning of a Perceptron. What are its limitations?\n",
        "Answer:\n",
        "\n",
        "\n",
        "A perceptron has inputs, weights, a bias, a summation function, and an activation function.\n",
        "It calculates output as:\n",
        "Output = Activation(Σ(weight × input) + bias)\n",
        "Limitations:\n",
        "\n",
        "Works only for linearly separable data\n",
        "\n",
        "Cannot solve complex problems like XOR\n",
        "\n",
        "Limited learning capacity\n",
        "\n",
        "Question 3: Describe the purpose of activation function in neural networks. Compare Sigmoid, ReLU, and Tanh functions.\n",
        "Answer:\n",
        "\n",
        "\n",
        "Activation functions introduce non-linearity in neural networks, helping them learn complex data.\n",
        "\n",
        "Sigmoid: Output between (0,1); used for probabilities; causes vanishing gradient.\n",
        "\n",
        "ReLU: Output = max(0, x); faster training; may cause dead neurons.\n",
        "\n",
        "Tanh: Output between (-1,1); better than Sigmoid but still suffers vanishing gradient.\n",
        "\n",
        "Question 4: What is the difference between Loss function and Cost function in neural networks? Provide examples.\n",
        "Answer:\n",
        "\n",
        "\n",
        "Loss Function: Measures error for a single training example.\n",
        "Example: Mean Squared Error, Cross-Entropy Loss.\n",
        "\n",
        "Cost Function: Average of all losses across training samples.\n",
        "Example: Mean of all sample losses in the dataset.\n",
        "\n",
        "Question 5: What is the role of optimizers in neural networks? Compare Gradient Descent, Adam, and RMSprop.\n",
        "Answer:\n",
        "\n",
        "Optimizers adjust network weights to minimize loss.\n",
        "\n",
        "Gradient Descent: Updates weights using full dataset; slow but stable.\n",
        "\n",
        "Adam: Combines momentum and adaptive learning rate; faster and efficient.\n",
        "\n",
        "RMSprop: Uses moving average of squared gradients; good for non-stationary problems.\n",
        "\n",
        "Question 6: Implement a single-layer perceptron for AND gate using NumPy.\n",
        "\n"
      ],
      "metadata": {
        "id": "M92mfpZFL5Cr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# AND gate data\n",
        "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
        "y = np.array([0,0,0,1])\n",
        "w = np.zeros(2)\n",
        "b = 0\n",
        "lr = 0.1\n",
        "\n",
        "for _ in range(10):\n",
        "    for i in range(len(X)):\n",
        "        y_pred = np.dot(X[i], w) + b\n",
        "        y_pred = 1 if y_pred > 0 else 0\n",
        "        w += lr * (y[i] - y_pred) * X[i]\n",
        "        b += lr * (y[i] - y_pred)\n",
        "print(\"Weights:\", w, \"Bias:\", b)\n"
      ],
      "metadata": {
        "id": "h7jqviQ5MWc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Implement and visualize Sigmoid, ReLU, and Tanh functions."
      ],
      "metadata": {
        "id": "b8d0imCJMbcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.linspace(-5, 5, 100)\n",
        "sigmoid = 1 / (1 + np.exp(-x))\n",
        "relu = np.maximum(0, x)\n",
        "tanh = np.tanh(x)\n",
        "\n",
        "plt.plot(x, sigmoid, label='Sigmoid')\n",
        "plt.plot(x, relu, label='ReLU')\n",
        "plt.plot(x, tanh, label='Tanh')\n",
        "plt.legend(); plt.show()\n"
      ],
      "metadata": {
        "id": "ECtB9IGMMd23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: Build and train a multilayer NN on MNIST using Keras."
      ],
      "metadata": {
        "id": "41v0ewnGMpzO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(-1, 784)/255\n",
        "x_test = x_test.reshape(-1, 784)/255\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=3, batch_size=32)\n",
        "print(\"Training Accuracy:\", model.evaluate(x_test, y_test)[1])\n"
      ],
      "metadata": {
        "id": "GFu9giJJMwwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Visualize loss and accuracy curves for Fashion MNIST."
      ],
      "metadata": {
        "id": "dclQjOFzMyZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "x_train, x_test = x_train/255.0, x_test/255.0\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, epochs=5, validation_split=0.2)\n",
        "\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.legend(); plt.show()\n"
      ],
      "metadata": {
        "id": "b_iRsaZTM1NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Fraud Detection Deep Learning Workflow"
      ],
      "metadata": {
        "id": "_pQC5wYGM5Uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models, layers\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Dense(64, activation='relu', input_shape=(features,)),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, validation_split=0.2, epochs=10, class_weight=weights)\n"
      ],
      "metadata": {
        "id": "QwNskBbnM8Lh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}